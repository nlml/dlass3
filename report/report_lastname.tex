\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{array, tabularx}
\usepackage{caption}
\usepackage{subcaption}

\title{Report for the Deep Learning Course Assignment 2 }

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
	Liam Schoneveld \\
	\texttt{liam.schoneveld@student.uva.nl}
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
In this assignment, we were tasked with implementing various convolutional neural network (CNN) architectures in TensorFlow (TF)...

\end{abstract}

\section{Task 1}



\begin{figure}[h]
	\centering
	\begin{subfigure}{.31\textwidth}
		\includegraphics[width=1.0\linewidth]{figures/mani_flat.png}
		\caption{Flat}
	\end{subfigure}%
	\begin{subfigure}{.31\textwidth}
		\includegraphics[width=1.0\linewidth]{figures/mani_fc1.png}
		\caption{FC1}
	\end{subfigure}%
	\begin{subfigure}{.37\textwidth}
		\includegraphics[width=1.0\linewidth]{figures/mani_fc2.png}
		\caption{FC2}
	\end{subfigure}
	\caption{t-SNE manifolds obtained from extracting linear convnet features from layers Flatten/FC1/FC2}
	\label{fig:tsnebasic}
\end{figure}

Bkah

\begin{figure}
	\centering
	\begin{subfigure}{.33\textwidth}
		\includegraphics[width=1.0\linewidth]{figures/accuracy_reg0e+00.png}
	\end{subfigure}%
	\begin{subfigure}{.33\textwidth}
		\includegraphics[width=1.0\linewidth]{figures/accuracy_reg1e-01.png}
	\end{subfigure}%
	\begin{subfigure}{.33\textwidth}
		\includegraphics[width=1.0\linewidth]{figures/accuracy_reg1e-02.png}
	\end{subfigure}
	\begin{subfigure}{.33\textwidth}
		\includegraphics[width=1.0\linewidth]{figures/accuracy_reg1e-03.png}
	\end{subfigure}%
	\begin{subfigure}{.33\textwidth}
		\includegraphics[width=1.0\linewidth]{figures/accuracy_reg1e-04.png}
	\end{subfigure}%
	\begin{subfigure}{.33\textwidth}
		\includegraphics[width=1.0\linewidth]{figures/accuracy_bar.png}
	\end{subfigure}
	\caption{Accuracy on test set over time using various dropout and weight regularisation settings (last figure shows max accuracy over all epochs for each setting)}
	\label{fig:lms}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/lm_acc.png}
	\caption{Performance of various dropout and weight regularisation settings on linear 1vsAll classifier model performance using features from flatten/FC1/FC2}
	\label{fig:lms}
\end{figure}


Use feature\_extraction() in order to compute features for test samples at layer fc2. Use these features to visualize learned space by the help of t-SNE. Include the visualization in your report.

Note: You can download and use the implementation for t-SNE from here. You can also use sklearn implementation of t-SNE.

Train 10 linear one-vs-rest classifiers for each class and report the performances. Can you draw similar conclusions by just looking at the visualization and thinking about separability and how good classes are represented by the model?

Note: You can use any linear classifier implementation available as long as you preserve consistency in experiments. See scikitlearn, libsvm, liblinear, VLFeat and etc.

Repeat the above experiments for flatten and fc1. Discuss the results. What is more suited for classification purposes: flatten, fc1 or fc2?

Can you improve test performance by using regularization techniques? Try L2 weight regularization on the fully-connected layers. You can also try others regularization techniques like dropout and batch normalization. Report your conclusions in your report with experimental support.



\section{Task 2}

"As clear from the definition of contrastive loss, we can see that this objective is not explicitly designed for classification purpose although it is yet a discriminative one. Identify one or two applications of such loss models."

SIAMESE TSNE VIS

SIAMESE 1VSREST PERF

\section{Task 3}



\section{Conclusion}
Should contain conclusion of this study.

\section*{References}

\small

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
  Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
  Exploring Realistic Neural Models with the GEneral NEural SImulation
  System.}  New York: TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
learning and recall at excitatory recurrent synapses and cholinergic
modulation in rat hippocampal region CA3. {\it Journal of
  Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
